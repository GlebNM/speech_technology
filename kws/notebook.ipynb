{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f88fd83",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f798cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ddde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./data/train_data/train_opus\"\n",
    "test_data_path = \"./data/test_data/test_opus\"\n",
    "train_audio_path = f\"{train_data_path}/audio\"\n",
    "test_audio_path = f\"{test_data_path}/audio\"\n",
    "train_bounds_path = f\"{train_data_path}/word_bounds.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160fd89",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711d048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180000/180000 [00:00<00:00, 1832934.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:  Counter({1: 45000, 0: 45000})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_audio_paths = []\n",
    "train_audio_labels = []\n",
    "\n",
    "with open(train_bounds_path, \"r\") as f:\n",
    "    train_bounds = json.load(f)\n",
    "\n",
    "for audio_path in tqdm(os.listdir(train_audio_path)):\n",
    "    if audio_path.startswith('.'):\n",
    "        continue\n",
    "    audio_id = audio_path.split('.')[0]\n",
    "    label = 1 if audio_id in train_bounds else 0\n",
    "    train_audio_labels.append(label)\n",
    "    train_audio_paths.append(os.path.join(train_audio_path, audio_path))\n",
    "\n",
    "print(\"Label distribution: \", Counter(train_audio_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282b02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_paths, val_audio_paths, train_audio_labels, val_audio_labels = train_test_split(\n",
    "    train_audio_paths,\n",
    "    train_audio_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=train_audio_labels,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282a202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordDataset(Dataset):\n",
    "    def __init__(self, audio_paths, audio_labels, transform=None, sr=16000):\n",
    "        self.audio_labels = audio_labels\n",
    "        self.transform = transform\n",
    "        self.sr = sr\n",
    "        self.data = []\n",
    "        \n",
    "        for audio_path in tqdm(audio_paths):\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            if sample_rate != self.sr:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sr)\n",
    "                waveform = resampler(waveform)\n",
    "            if transform:\n",
    "                waveform = transform(waveform)\n",
    "            self.data.append(waveform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.audio_labels[idx]\n",
    "        waveform = self.data[idx]\n",
    "        # if self.transform:\n",
    "        #     waveform = self.transform(waveform)\n",
    "        return waveform, label\n",
    "    \n",
    "mel_spectrogram_transform = nn.Sequential(\n",
    "    MelSpectrogram(sample_rate=16000, n_mels=64),\n",
    "    AmplitudeToDB()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78482f21",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3db411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1904.03814\n",
    "\n",
    "class TemporalResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=9):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_ch, out_ch, kernel_size,\n",
    "            padding=padding, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_ch, out_ch, kernel_size,\n",
    "            padding=padding, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_ch, out_ch, kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        skip = self.conv3(skip)\n",
    "        skip = self.bn3(skip)\n",
    "        skip = self.relu(skip)\n",
    "\n",
    "        out += skip\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class TCResNet8(nn.Module):\n",
    "    def __init__(self, num_classes=1, channels=(16, 24, 32, 48)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_conv = nn.Sequential(\n",
    "            nn.Conv1d(64, channels[0], kernel_size=9, padding=4, bias=False),\n",
    "            nn.BatchNorm1d(channels[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for channel_idx in range(len(channels) - 1):\n",
    "            self.layers.append(TemporalResBlock(channels[channel_idx], channels[channel_idx + 1]))\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3bf074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:  71937\n"
     ]
    }
   ],
   "source": [
    "model = TCResNet8()\n",
    "\n",
    "print(\"Total params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ef5a6",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c79eef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_metrics(preds, targets):\n",
    "    preds = (torch.sigmoid(preds) > 0.5).float()\n",
    "    acc = (preds == targets).float().mean().item()\n",
    "    fn = (preds < targets).int().sum().item()\n",
    "    fp = (preds > targets).int().sum().item()\n",
    "    pos = targets.int().sum().item()\n",
    "    neg = targets.shape[0] - pos\n",
    "    f1 = 2 * ((pos - fn) / (2 * pos - fn + fp)) if (2 * pos - fn + fp) > 0 else 0.0\n",
    "    far = fp / neg if neg > 0 else 0.0\n",
    "    frr = fn / pos if pos > 0 else 0.0\n",
    "    score = 2 * ((1 - frr) * (1 - far)) / ((1 - frr) + (1 - far)) if ((1 - frr) + (1 - far)) > 0 else 0.0\n",
    " \n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "def eval_step(model, test_dataloader, criterion, device='cuda', max_steps=None):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    steps = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader, desc=\"Validation\", leave=False)):\n",
    "            if max_steps is not None and i >= max_steps:\n",
    "                break\n",
    "            mels, labels = batch\n",
    "            mels = mels.to(device)\n",
    "            labels = labels.float().to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(mels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            targets = labels.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            steps += 1\n",
    "    \n",
    "    if steps > 0:\n",
    "        valid_loss /= steps\n",
    "        \n",
    "    metrics = get_metrics(torch.tensor(all_preds), torch.tensor(all_targets))\n",
    "    metrics['loss'] = valid_loss\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_loop(model, train_dataloader, val_dataloader, epochs, optimizer, criterion, val_every=100, device='cuda', log_dir='./logs', best_path=None, val_steps=None):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    global_step = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_score = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            model.train()\n",
    "            mels, labels = batch\n",
    "            mels = mels.to(device)\n",
    "            labels = labels.float().to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            metrics = get_metrics(outputs, labels)\n",
    "            for key, value in metrics.items():\n",
    "                writer.add_scalar(f'Train/{key}', value, global_step)\n",
    "            \n",
    "            writer.add_scalar('Train/Batch_Loss', loss.item(), global_step)\n",
    "            global_step += 1\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': metrics['Accuracy'], 'score': metrics['score']})\n",
    "            \n",
    "            if val_every and i % val_every == 0 and i > 0:\n",
    "                metrics = eval_step(model, val_dataloader, criterion, device=device, max_steps=val_steps)\n",
    "                writer.add_scalar('Valid/Loss', metrics['loss'], global_step)\n",
    "                writer.add_scalar('Valid/Accuracy', metrics['Accuracy'], global_step)\n",
    "                writer.add_scalar('Valid/F1', metrics['f1'], global_step)\n",
    "                writer.add_scalar('Valid/Score', metrics['score'], global_step)\n",
    "                \n",
    "        train_loss /= len(train_dataloader)\n",
    "        metrics = eval_step(model, val_dataloader, criterion, device=device)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Valid Loss: {metrics['loss']:.4f}\", f\"Valid Acc: {metrics['Accuracy']:.4f}\", f\"Valid F1: {metrics['f1']:.4f}\", f\"Valid Score: {metrics['score']:.4f}\")\n",
    "        writer.add_scalar('Train/Epoch_Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Valid/Epoch_Loss', metrics['loss'], epoch)\n",
    "        writer.add_scalar('Valid/Epoch_Accuracy', metrics['Accuracy'], epoch)\n",
    "        writer.add_scalar('Valid/Epoch_F1', metrics['f1'], epoch)\n",
    "        writer.add_scalar('Valid/Epoch_Score', metrics['score'], epoch)\n",
    "        \n",
    "        if best_path is not None and metrics['score'] > best_val_score:\n",
    "            best_val_score = metrics['score']\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"New best model saved\")\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14cc7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_pkl_path = \"train_dataset.pkl\"\n",
    "val_pkl_path = \"val_dataset.pkl\"\n",
    "\n",
    "# train_dataset = KeywordDataset(train_audio_paths, train_audio_labels, transform=mel_spectrogram_transform)\n",
    "# val_dataset = KeywordDataset(val_audio_paths, val_audio_labels, transform=mel_spectrogram_transform)\n",
    "\n",
    "# with open(train_pkl_path, \"wb\") as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "# with open(val_pkl_path, \"wb\") as f:\n",
    "#     pickle.dump(val_dataset, f)\n",
    "\n",
    "\n",
    "with open(train_pkl_path, \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "with open(val_pkl_path, \"rb\") as f:\n",
    "    val_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7180e8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c9abc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 1125/1125 [00:07<00:00, 141.59it/s, loss=0.457, acc=0.781, score=0.78] \n",
      "Epoch 1/25: 100%|██████████| 1125/1125 [00:07<00:00, 141.59it/s, loss=0.457, acc=0.781, score=0.78]\n",
      "/tmp/ipykernel_196438/3841939571.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  metrics = get_metrics(torch.tensor(all_preds), torch.tensor(all_targets))\n",
      "/tmp/ipykernel_196438/3841939571.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  metrics = get_metrics(torch.tensor(all_preds), torch.tensor(all_targets))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.5751, Valid Loss: 0.4852 Valid Acc: 0.7761 Valid F1: 0.7587 Valid Score: 0.7694\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 1125/1125 [00:07<00:00, 158.04it/s, loss=0.254, acc=0.875, score=0.874]\n",
      "Epoch 2/25: 100%|██████████| 1125/1125 [00:07<00:00, 158.04it/s, loss=0.254, acc=0.875, score=0.874]\n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Train Loss: 0.4322, Valid Loss: 0.3871 Valid Acc: 0.8313 Valid F1: 0.8224 Valid Score: 0.8283\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 1125/1125 [00:07<00:00, 159.44it/s, loss=0.316, acc=0.906, score=0.906]\n",
      "Epoch 3/25: 100%|██████████| 1125/1125 [00:07<00:00, 159.44it/s, loss=0.316, acc=0.906, score=0.906]\n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Train Loss: 0.3714, Valid Loss: 0.4483 Valid Acc: 0.7867 Valid F1: 0.7443 Valid Score: 0.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 1125/1125 [00:07<00:00, 160.04it/s, loss=0.375, acc=0.828, score=0.827]\n",
      "Epoch 4/25: 100%|██████████| 1125/1125 [00:07<00:00, 160.04it/s, loss=0.375, acc=0.828, score=0.827]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Train Loss: 0.3427, Valid Loss: 0.3277 Valid Acc: 0.8666 Valid F1: 0.8657 Valid Score: 0.8666\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 1125/1125 [00:07<00:00, 160.46it/s, loss=0.304, acc=0.859, score=0.852]\n",
      "Epoch 5/25: 100%|██████████| 1125/1125 [00:07<00:00, 160.46it/s, loss=0.304, acc=0.859, score=0.852]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Train Loss: 0.3203, Valid Loss: 0.3315 Valid Acc: 0.8652 Valid F1: 0.8617 Valid Score: 0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 1125/1125 [00:07<00:00, 156.26it/s, loss=0.297, acc=0.828, score=0.836]\n",
      "Epoch 6/25: 100%|██████████| 1125/1125 [00:07<00:00, 156.26it/s, loss=0.297, acc=0.828, score=0.836]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Train Loss: 0.3033, Valid Loss: 0.3791 Valid Acc: 0.8260 Valid F1: 0.7994 Valid Score: 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.26it/s, loss=0.371, acc=0.859, score=0.856]\n",
      "Epoch 7/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.26it/s, loss=0.371, acc=0.859, score=0.856]\n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 0.2890, Valid Loss: 0.3002 Valid Acc: 0.8770 Valid F1: 0.8729 Valid Score: 0.8758\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.29it/s, loss=0.358, acc=0.859, score=0.858]\n",
      "Epoch 8/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.29it/s, loss=0.358, acc=0.859, score=0.858]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Train Loss: 0.2789, Valid Loss: 0.3191 Valid Acc: 0.8681 Valid F1: 0.8591 Valid Score: 0.8634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.43it/s, loss=0.372, acc=0.891, score=0.89] \n",
      "Epoch 9/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.43it/s, loss=0.372, acc=0.891, score=0.89] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Train Loss: 0.2700, Valid Loss: 0.2895 Valid Acc: 0.8886 Valid F1: 0.8889 Valid Score: 0.8886\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 1125/1125 [00:07<00:00, 148.49it/s, loss=0.187, acc=0.953, score=0.959]\n",
      "Epoch 10/25: 100%|██████████| 1125/1125 [00:07<00:00, 148.49it/s, loss=0.187, acc=0.953, score=0.959]\n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Train Loss: 0.2584, Valid Loss: 0.2974 Valid Acc: 0.8894 Valid F1: 0.8920 Valid Score: 0.8887\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.11it/s, loss=0.325, acc=0.797, score=0.803]\n",
      "Epoch 11/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.11it/s, loss=0.325, acc=0.797, score=0.803]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Train Loss: 0.2529, Valid Loss: 0.2911 Valid Acc: 0.8908 Valid F1: 0.8931 Valid Score: 0.8903\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.31it/s, loss=0.2, acc=0.906, score=0.908]  \n",
      "Epoch 12/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.31it/s, loss=0.2, acc=0.906, score=0.908]  \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Train Loss: 0.2457, Valid Loss: 0.2727 Valid Acc: 0.8927 Valid F1: 0.8922 Valid Score: 0.8927\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.04it/s, loss=0.171, acc=0.938, score=0.931]\n",
      "\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 0.2401, Valid Loss: 0.2672 Valid Acc: 0.8955 Valid F1: 0.8940 Valid Score: 0.8953\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.39it/s, loss=0.173, acc=0.969, score=0.973] \n",
      "Epoch 14/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.39it/s, loss=0.173, acc=0.969, score=0.973]\n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Train Loss: 0.2339, Valid Loss: 0.2894 Valid Acc: 0.8814 Valid F1: 0.8756 Valid Score: 0.8789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.29it/s, loss=0.365, acc=0.766, score=0.766] \n",
      "Epoch 15/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.29it/s, loss=0.365, acc=0.766, score=0.766]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Train Loss: 0.2260, Valid Loss: 0.2721 Valid Acc: 0.8982 Valid F1: 0.8994 Valid Score: 0.8981\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.15it/s, loss=0.204, acc=0.938, score=0.947]\n",
      "Epoch 16/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.15it/s, loss=0.204, acc=0.938, score=0.947]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Train Loss: 0.2214, Valid Loss: 0.2677 Valid Acc: 0.8949 Valid F1: 0.8941 Valid Score: 0.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 1125/1125 [00:07<00:00, 148.89it/s, loss=0.241, acc=0.891, score=0.891] \n",
      "Epoch 17/25: 100%|██████████| 1125/1125 [00:07<00:00, 148.89it/s, loss=0.241, acc=0.891, score=0.891]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Train Loss: 0.2149, Valid Loss: 0.3641 Valid Acc: 0.8815 Valid F1: 0.8893 Valid Score: 0.8758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.98it/s, loss=0.26, acc=0.906, score=0.901] \n",
      "Epoch 18/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.98it/s, loss=0.26, acc=0.906, score=0.901] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Train Loss: 0.2103, Valid Loss: 0.2633 Valid Acc: 0.8997 Valid F1: 0.8986 Valid Score: 0.8995\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.57it/s, loss=0.294, acc=0.859, score=0.86]  \n",
      "Epoch 19/25: 100%|██████████| 1125/1125 [00:07<00:00, 151.57it/s, loss=0.294, acc=0.859, score=0.86] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.2029, Valid Loss: 0.2789 Valid Acc: 0.8922 Valid F1: 0.8926 Valid Score: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.91it/s, loss=0.163, acc=0.938, score=0.935] \n",
      "Epoch 20/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.91it/s, loss=0.163, acc=0.938, score=0.935]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Train Loss: 0.1990, Valid Loss: 0.2601 Valid Acc: 0.8982 Valid F1: 0.8959 Valid Score: 0.8976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 1125/1125 [00:07<00:00, 148.04it/s, loss=0.37, acc=0.906, score=0.915]  \n",
      "Epoch 21/25: 100%|██████████| 1125/1125 [00:07<00:00, 148.04it/s, loss=0.37, acc=0.906, score=0.915] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Train Loss: 0.1957, Valid Loss: 0.2584 Valid Acc: 0.9018 Valid F1: 0.9010 Valid Score: 0.9018\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.48it/s, loss=0.22, acc=0.938, score=0.935]  \n",
      "Epoch 22/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.48it/s, loss=0.22, acc=0.938, score=0.935] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Train Loss: 0.1909, Valid Loss: 0.2732 Valid Acc: 0.8951 Valid F1: 0.8921 Valid Score: 0.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 1125/1125 [00:07<00:00, 145.70it/s, loss=0.182, acc=0.922, score=0.921] \n",
      "Epoch 23/25: 100%|██████████| 1125/1125 [00:07<00:00, 145.70it/s, loss=0.182, acc=0.922, score=0.921]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Train Loss: 0.1853, Valid Loss: 0.2757 Valid Acc: 0.8902 Valid F1: 0.8863 Valid Score: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.38it/s, loss=0.164, acc=0.953, score=0.96]  \n",
      "Epoch 24/25: 100%|██████████| 1125/1125 [00:07<00:00, 150.38it/s, loss=0.164, acc=0.953, score=0.96] \n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Train Loss: 0.1838, Valid Loss: 0.3178 Valid Acc: 0.8922 Valid F1: 0.8961 Valid Score: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.08it/s, loss=0.13, acc=0.953, score=0.953]  \n",
      "Epoch 25/25: 100%|██████████| 1125/1125 [00:07<00:00, 149.08it/s, loss=0.13, acc=0.953, score=0.953]\n",
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.1792, Valid Loss: 0.2788 Valid Acc: 0.9029 Valid F1: 0.9043 Valid Score: 0.9026\n",
      "New best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "lr = 1e-3\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = TCResNet8().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loop(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=epochs,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    best_path=\"best_model.pth\",\n",
    "    val_every=None,\n",
    "    val_steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d853fc3",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c108c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54000/54000 [00:00<00:00, 2029429.20it/s]\n",
      "100%|██████████| 54000/54000 [00:00<00:00, 2029429.20it/s]\n",
      "100%|██████████| 27000/27000 [03:39<00:00, 123.03it/s]\n",
      "100%|██████████| 27000/27000 [03:39<00:00, 123.03it/s]\n",
      "100%|██████████| 422/422 [00:01<00:00, 344.09it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "\n",
    "test_audio_paths = []\n",
    "test_audio_ids = []\n",
    "\n",
    "sorted_files = sorted(os.listdir(test_audio_path))\n",
    "\n",
    "for audio_path in tqdm(sorted_files):\n",
    "    if audio_path.startswith('.'):\n",
    "        continue\n",
    "    audio_id = audio_path.split('.')[0]\n",
    "    test_audio_ids.append(audio_id)\n",
    "    test_audio_paths.append(os.path.join(test_audio_path, audio_path))\n",
    "\n",
    "test_audio_labels = [0] * len(test_audio_paths)\n",
    "\n",
    "test_dataset = KeywordDataset(test_audio_paths, test_audio_labels, transform=mel_spectrogram_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        mels, _ = batch\n",
    "        mels = mels.to(device)\n",
    "        outputs = model(mels)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int().cpu().numpy().flatten()\n",
    "        predictions.extend(preds)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'id': test_audio_ids,\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07873e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
